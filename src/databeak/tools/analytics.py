# Apache License
# Version 2.0, January 2004
# http://www.apache.org/licenses/
#
# TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
#
# 1. Definitions.
#
# "License" shall mean the terms and conditions for use, reproduction,
# and distribution as defined by Sections 1 through 9 of this document.
#
# "Licensor" shall mean the copyright owner or entity granting the License.
#
# "Legal Entity" shall mean the union of the acting entity and all
# other entities that control, are controlled by, or are under common
# control with that entity. For the purposes of this definition,
# "control" means (i) the power, direct or indirect, to cause the
# direction or management of such entity, whether by contract or
# otherwise, or (ii) ownership of fifty percent (50%) or more of the
# outstanding shares, or (iii) beneficial ownership of such entity.
#
# "You" (or "Your") shall mean an individual or Legal Entity
# exercising permissions granted by this License.
#
# "Source" form shall mean the preferred form for making modifications,
# including but not limited to software source code, documentation
# source, and configuration files.
#
# "Object" form shall mean any form resulting from mechanical
# transformation or translation of a Source form, including but
# not limited to compiled object code, generated documentation,
# and conversions to other media types.
#
# "Work" shall mean the work of authorship, whether in Source or
# Object form, made available under the License, as indicated by a
# copyright notice that is included in or attached to the work
# (which shall not include communications that are individually
# marked or otherwise designated in writing by the copyright owner
# as "Not a Contribution").
#
# "Derivative Works" shall mean any work, whether in Source or Object
# form, that is based upon (or derived from) the Work and for which the
# editorial revisions, annotations, elaborations, or other modifications
# represent, as a whole, an original work of authorship. For the purposes
# of this License, Derivative Works shall not include works that remain
# separable from, or merely link (or bind by name) to the interfaces of,
# the Work and derivative works thereof.
#
# "Contribution" shall mean any work of authorship, including
# the original version of the Work and any modifications or additions
# to that Work or Derivative Works thereof, that is intentionally
# submitted to Licensor for inclusion in the Work by the copyright owner
# or by an individual or Legal Entity authorized to submit on behalf of
# the copyright owner. For the purposes of this definition, "submitted"
# means any form of electronic, verbal, or written communication sent
# to the Licensor or its representatives, including but not limited to
# communication on electronic mailing lists, source code control
# systems, and issue tracking systems that are managed by, or on behalf
# of, the Licensor for the purpose of discussing and improving the Work,
# but excluding communication that is conspicuously marked or otherwise
# designated in writing by the copyright owner as "Not a Contribution."
#
# 2. Grant of Copyright License. Subject to the terms and conditions of
# this License, each Contributor hereby grants to You a perpetual,
# worldwide, non-exclusive, no-charge, royalty-free, irrevocable
# copyright license to use, reproduce, modify, display, perform,
# sublicense, and distribute the Work and such Derivative Works in
# Source or Object form.
#
# 3. Grant of Patent License. Subject to the terms and conditions of
# this License, each Contributor hereby grants to You a perpetual,
# worldwide, non-exclusive, no-charge, royalty-free, irrevocable
# (except as stated in this section) patent license to make, have made,
# use, offer to sell, sell, import, and otherwise transfer the Work,
# where such license applies only to those patent claims licensable
# by such Contributor that are necessarily infringed by their
# Contribution(s) alone or by combination of their Contribution(s)
# with the Work to which such Contribution(s) was submitted. If You
# institute patent litigation against any entity (including a
# cross-claim or counterclaim in a lawsuit) alleging that the Work
# or a Contribution incorporated within the Work constitutes direct
# or contributory patent infringement, then any patent licenses
# granted to You under this License for that Work shall terminate
# as of the date such litigation is filed.
#
# 4. Redistribution. You may reproduce and distribute copies of the
# Work or Derivative Works thereof in any medium, with or without
# modifications, and in Source or Object form, provided that You
# meet the following conditions:
#
# (a) You must give any other recipients of the Work or
# Derivative Works a copy of this License; and
#
# (b) You must cause any modified files to carry prominent notices
# stating that You changed the files; and
#
# (c) You must retain, in the Source form of any Derivative Works
# that You distribute, all copyright, patent, trademark, and
# attribution notices from the Source form of the Work,
# excluding those notices that do not pertain to any part of
# the Derivative Works; and
#
# (d) If the Work includes a "NOTICE" text file as part of its
# distribution, then any Derivative Works that You distribute must
# include a readable copy of the attribution notices contained
# within such NOTICE file, excluding those notices that do not
# pertain to any part of the Derivative Works, in at least one
# of the following places: within a NOTICE text file distributed
# as part of the Derivative Works; within the Source form or
# documentation, if provided along with the Derivative Works; or,
# within a display generated by the Derivative Works, if and
# wherever such third-party notices normally appear. The contents
# of the NOTICE file are for informational purposes only and
# do not modify the License. You may add Your own attribution
# notices within Derivative Works that You distribute, alongside
# or as an addendum to the NOTICE text from the Work, provided
# that such additional attribution notices cannot be construed
# as modifying the License.
#
# You may add Your own copyright notice to Your modifications and
# may provide additional or different license terms and conditions
# for use, reproduction, or distribution of Your modifications, or
# for any such Derivative Works as a whole, provided Your use,
# reproduction, and distribution of the Work otherwise complies with
# the conditions stated in this License.
#
# 5. Submission of Contributions. Unless You explicitly state otherwise,
# any Contribution intentionally submitted for inclusion in the Work
# by You to the Licensor shall be under the terms and conditions of
# this License, without any additional terms or conditions.
# Notwithstanding the above, nothing herein shall supersede or modify
# the terms of any separate license agreement you may have executed
# with Licensor regarding such Contributions.
#
# 6. Trademarks. This License does not grant permission to use the trade
# names, trademarks, service marks, or product names of the Licensor,
# except as required for reasonable and customary use in describing the
# origin of the Work and reproducing the content of the NOTICE file.
#
# 7. Disclaimer of Warranty. Unless required by applicable law or
# agreed to in writing, Licensor provides the Work (and each
# Contributor provides its Contributions) on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
# implied, including, without limitation, any warranties or conditions
# of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
# PARTICULAR PURPOSE. You are solely responsible for determining the
# appropriateness of using or redistributing the Work and assume any
# risks associated with Your exercise of permissions under this License.
#
# 8. Limitation of Liability. In no event and under no legal theory,
# whether in tort (including negligence), contract, or otherwise,
# unless required by applicable law (such as deliberate and grossly
# negligent acts) or agreed to in writing, shall any Contributor be
# liable to You for damages, including any direct, indirect, special,
# incidental, or consequential damages of any character arising as a
# result of this License or out of the use or inability to use the
# Work (including but not limited to damages for loss of goodwill,
# work stoppage, computer failure or malfunction, or any and all
# other commercial damages or losses), even if such Contributor
# has been advised of the possibility of such damages.
#
# 9. Accepting Warranty or Support. While redistributing the Work or
# Derivative Works thereof, You may choose to offer, and charge a fee
# for, acceptance of support, warranty, indemnity, or other liability
# obligations and/or rights consistent with this License. However, in
# accepting such obligations, You may act only on Your own behalf and on
# Your sole responsibility, not on behalf of any other Contributor, and
# only if You agree to indemnify, defend, and hold each Contributor
# harmless for any liability incurred by, or claims asserted against,
# such Contributor by reason of your accepting any such warranty or support.
#
# END OF TERMS AND CONDITIONS
#
# APPENDIX: How to apply the Apache License to your work.
#
# To apply the Apache License to your work, attach the following
# boilerplate notice, with the fields enclosed by brackets "[]"
# replaced with your own identifying information. (Don't include
# the brackets!)  The text should be enclosed in the appropriate
# comment syntax for the file format. We also recommend that a
# file or class name and description of purpose be included on the
# same "printed page" as the copyright notice for easier
# identification within third-party archives.
#
# Copyright [yyyy] [name of copyright owner]
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Analytics tools for CSV data analysis."""

from __future__ import annotations

import logging
from typing import TYPE_CHECKING, Any, Literal, cast

import numpy as np
import pandas as pd

from ..models.csv_session import get_session_manager
from ..models.data_models import OperationType

if TYPE_CHECKING:
    from fastmcp import Context

logger = logging.getLogger(__name__)


async def get_statistics(
    session_id: str,
    columns: list[str] | None = None,
    include_percentiles: bool = True,
    ctx: Context | None = None,  # noqa: ARG001
) -> dict[str, Any]:
    """Get statistical summary of numerical columns.

    Args:
        session_id: Session identifier
        columns: Specific columns to analyze (None for all numeric)
        include_percentiles: Include percentile values
        ctx: FastMCP context

    Returns:
        Dict with statistics for each column
    """
    try:
        manager = get_session_manager()
        session = manager.get_session(session_id)

        if not session or not session.data_session.has_data():
            return {"success": False, "error": "Invalid session or no data loaded"}

        df = session.data_session.df

        # Select columns to analyze
        if columns:
            missing_cols = [col for col in columns if col not in df.columns]
            if missing_cols:
                return {"success": False, "error": f"Columns not found: {missing_cols}"}
            numeric_df = df[columns].select_dtypes(include=[np.number])
        else:
            numeric_df = df.select_dtypes(include=[np.number])

        if numeric_df.empty:
            # Return basic statistics for empty numeric data
            return {
                "success": True,
                "statistics": {
                    "row_count": len(df),
                    "column_count": len(df.columns),
                    "numeric_columns": [],
                    "non_numeric_columns": list(df.columns),
                },
                "session_id": session_id,
            }

        # Calculate statistics
        stats = {}

        for col in numeric_df.columns:
            col_data = numeric_df[col].dropna()

            col_stats = {
                "count": int(col_data.count()),
                "null_count": int(df[col].isna().sum()),
                "mean": float(col_data.mean()),
                "std": float(col_data.std()),
                "min": float(col_data.min()),
                "max": float(col_data.max()),
                "sum": float(col_data.sum()),
                "variance": float(cast("float", col_data.var())),
                "skewness": float(cast("float", col_data.skew())),
                "kurtosis": float(cast("float", col_data.kurtosis())),
            }

            if include_percentiles:
                col_stats["25%"] = float(col_data.quantile(0.25))
                col_stats["50%"] = float(col_data.quantile(0.50))
                col_stats["75%"] = float(col_data.quantile(0.75))
                col_stats["iqr"] = col_stats["75%"] - col_stats["25%"]

            stats[col] = col_stats

        session.record_operation(
            OperationType.ANALYZE, {"type": "statistics", "columns": list(stats.keys())}
        )

        return {
            "success": True,
            "statistics": stats,
            "columns_analyzed": list(stats.keys()),
            "total_rows": len(df),
        }

    except Exception as e:
        logger.error(f"Error getting statistics: {e!s}")
        return {"success": False, "error": str(e)}


async def get_column_statistics(
    session_id: str,
    column: str,
    ctx: Context | None = None,  # noqa: ARG001
) -> dict[str, Any]:
    """Get detailed statistics for a specific column.

    Args:
        session_id: Session identifier
        column: Column name to analyze
        ctx: FastMCP context

    Returns:
        Dict with detailed column statistics
    """
    try:
        manager = get_session_manager()
        session = manager.get_session(session_id)

        if not session or not session.data_session.has_data():
            return {"success": False, "error": "Invalid session or no data loaded"}

        df = session.data_session.df

        if column not in df.columns:
            return {"success": False, "error": f"Column '{column}' not found"}

        col_data = df[column]
        result = {
            "column": column,
            "dtype": str(col_data.dtype),
            "total_count": len(col_data),
            "null_count": int(col_data.isna().sum()),
            "null_percentage": round(col_data.isna().sum() / len(col_data) * 100, 2),
            "unique_count": int(col_data.nunique()),
            "unique_percentage": round(col_data.nunique() / len(col_data) * 100, 2),
        }

        # Numeric column statistics
        if pd.api.types.is_numeric_dtype(col_data):
            non_null = col_data.dropna()
            result.update(
                {
                    "type": "numeric",
                    "mean": float(non_null.mean()),
                    "median": float(non_null.median()),
                    "mode": (float(non_null.mode()[0]) if len(non_null.mode()) > 0 else None),
                    "std": float(non_null.std()),
                    "variance": float(cast("float", non_null.var())),
                    "min": float(non_null.min()),
                    "max": float(non_null.max()),
                    "range": float(non_null.max() - non_null.min()),
                    "sum": float(non_null.sum()),
                    "skewness": float(cast("float", non_null.skew())),
                    "kurtosis": float(cast("float", non_null.kurtosis())),
                    "25%": float(non_null.quantile(0.25)),
                    "50%": float(non_null.quantile(0.50)),
                    "75%": float(non_null.quantile(0.75)),
                    "iqr": float(non_null.quantile(0.75) - non_null.quantile(0.25)),
                    "zero_count": int((col_data == 0).sum()),
                    "positive_count": int((col_data > 0).sum()),
                    "negative_count": int((col_data < 0).sum()),
                }
            )

        # Categorical column statistics
        else:
            value_counts = col_data.value_counts()
            top_values = value_counts.head(10).to_dict()

            result.update(
                {
                    "type": "categorical",
                    "most_frequent": (
                        str(value_counts.index[0]) if len(value_counts) > 0 else None
                    ),
                    "most_frequent_count": (
                        int(value_counts.iloc[0]) if len(value_counts) > 0 else 0
                    ),
                    "top_10_values": {str(k): int(v) for k, v in top_values.items()},
                }
            )

            # String-specific stats
            if col_data.dtype == "object":
                str_data = col_data.dropna().astype(str)
                if len(str_data) > 0:
                    str_lengths = str_data.str.len()
                    result["string_stats"] = {
                        "min_length": int(str_lengths.min()),
                        "max_length": int(str_lengths.max()),
                        "mean_length": round(str_lengths.mean(), 2),
                        "empty_string_count": int((str_data == "").sum()),
                    }

        session.record_operation(
            OperationType.ANALYZE, {"type": "column_statistics", "column": column}
        )

        return {"success": True, "statistics": result}

    except Exception as e:
        logger.error(f"Error getting column statistics: {e!s}")
        return {"success": False, "error": str(e)}


async def get_correlation_matrix(
    session_id: str,
    method: Literal["pearson", "spearman", "kendall"] = "pearson",
    columns: list[str] | None = None,
    min_correlation: float | None = None,
    ctx: Context | None = None,  # noqa: ARG001
) -> dict[str, Any]:
    """Calculate correlation matrix for numeric columns.

    Args:
        session_id: Session identifier
        method: Correlation method ('pearson', 'spearman', 'kendall')
        columns: Specific columns to include (None for all numeric)
        min_correlation: Filter to show only correlations above this threshold
        ctx: FastMCP context

    Returns:
        Dict with correlation matrix
    """
    try:
        manager = get_session_manager()
        session = manager.get_session(session_id)

        if not session or not session.data_session.has_data():
            return {"success": False, "error": "Invalid session or no data loaded"}

        df = session.data_session.df

        # Select columns
        if columns:
            missing_cols = [col for col in columns if col not in df.columns]
            if missing_cols:
                return {"success": False, "error": f"Columns not found: {missing_cols}"}
            numeric_df = df[columns].select_dtypes(include=[np.number])
        else:
            numeric_df = df.select_dtypes(include=[np.number])

        if numeric_df.empty:
            return {"success": False, "error": "No numeric columns found"}

        if len(numeric_df.columns) < 2:
            return {
                "success": False,
                "error": "Need at least 2 numeric columns for correlation",
            }

        # Calculate correlation
        if method not in ["pearson", "spearman", "kendall"]:
            return {"success": False, "error": f"Invalid method: {method}"}

        corr_matrix = numeric_df.corr(method=method)

        # Convert to dict format
        correlations: dict[str, dict[str, float]] = {}
        for col1 in corr_matrix.columns:
            correlations[col1] = {}
            for col2 in corr_matrix.columns:
                value = corr_matrix.loc[col1, col2]
                if not pd.isna(value):
                    float_value = float(cast("float", value))
                    if (
                        min_correlation is None
                        or abs(float_value) >= min_correlation
                        or col1 == col2
                    ):
                        correlations[col1][col2] = round(float_value, 4)

        # Find highly correlated pairs
        high_correlations = []
        # Use min_correlation if specified, otherwise use 0.7 as default threshold
        correlation_threshold = min_correlation if min_correlation is not None else 0.7

        for i, col1 in enumerate(corr_matrix.columns):
            for col2 in corr_matrix.columns[i + 1 :]:
                corr_value = corr_matrix.loc[col1, col2]
                if not pd.isna(corr_value):
                    float_corr = float(cast("float", corr_value))
                    if abs(float_corr) >= correlation_threshold:
                        high_correlations.append(
                            {
                                "column1": col1,
                                "column2": col2,
                                "correlation": round(float_corr, 4),
                            }
                        )

        high_correlations.sort(key=lambda x: abs(cast("float", x["correlation"])), reverse=True)

        session.record_operation(
            OperationType.ANALYZE,
            {
                "type": "correlation",
                "method": method,
                "columns": list(corr_matrix.columns),
            },
        )

        return {
            "success": True,
            "method": method,
            "correlation_matrix": correlations,
            "significant_correlations": high_correlations,
            "columns_analyzed": list(corr_matrix.columns),
        }

    except Exception as e:
        logger.error(f"Error calculating correlation: {e!s}")
        return {"success": False, "error": str(e)}


async def group_by_aggregate(
    session_id: str,
    group_by: list[str],
    aggregations: dict[str, str | list[str]],
    ctx: Context | None = None,  # noqa: ARG001
) -> dict[str, Any]:
    """Group data and apply aggregation functions.

    Args:
        session_id: Session identifier
        group_by: Columns to group by
        aggregations: Dict mapping column names to aggregation functions
                     e.g., {"sales": ["sum", "mean"], "quantity": "sum"}
        ctx: FastMCP context

    Returns:
        Dict with grouped data
    """
    try:
        manager = get_session_manager()
        session = manager.get_session(session_id)

        if not session or not session.data_session.has_data():
            return {"success": False, "error": "Invalid session or no data loaded"}

        df = session.data_session.df

        # Validate group by columns
        missing_cols = [col for col in group_by if col not in df.columns]
        if missing_cols:
            return {
                "success": False,
                "error": f"Group by columns not found: {missing_cols}",
            }

        # Validate aggregation columns
        agg_cols = list(aggregations.keys())
        missing_agg_cols = [col for col in agg_cols if col not in df.columns]
        if missing_agg_cols:
            return {
                "success": False,
                "error": f"Aggregation columns not found: {missing_agg_cols}",
            }

        # Prepare aggregation dict
        agg_dict: dict[str, Any] = {}
        for col, funcs in aggregations.items():
            if isinstance(funcs, str):
                agg_dict[col] = [funcs]
            else:
                agg_dict[col] = funcs

        # Perform groupby
        grouped = df.groupby(group_by).agg(agg_dict)

        # Flatten column names
        grouped.columns = [
            "_".join(col).strip() if col[1] else col[0] for col in grouped.columns.values
        ]

        # Reset index to make group columns regular columns
        result_df = grouped.reset_index()

        # Convert to dict for response
        result = {
            "data": result_df.to_dict(orient="records"),
            "shape": {"rows": len(result_df), "columns": len(result_df.columns)},
            "columns": result_df.columns.tolist(),
        }

        # Store grouped data in session
        session.data_session.df = result_df
        session.record_operation(
            OperationType.GROUP_BY,
            {
                "group_by": group_by,
                "aggregations": aggregations,
                "result_shape": result["shape"],
            },
        )

        return {
            "success": True,
            "grouped_data": result,
            "group_by": group_by,
            "aggregations": aggregations,
        }

    except Exception as e:
        logger.error(f"Error in group by aggregate: {e!s}")
        return {"success": False, "error": str(e)}


async def get_value_counts(
    session_id: str,
    column: str,
    normalize: bool = False,
    sort: bool = True,
    ascending: bool = False,
    top_n: int | None = None,
    ctx: Context | None = None,  # noqa: ARG001
) -> dict[str, Any]:
    """Get value counts for a column.

    Args:
        session_id: Session identifier
        column: Column name to count values
        normalize: Return proportions instead of counts
        sort: Sort by frequency
        ascending: Sort order
        top_n: Return only top N values
        ctx: FastMCP context

    Returns:
        Dict with value counts
    """
    try:
        manager = get_session_manager()
        session = manager.get_session(session_id)

        if not session or not session.data_session.has_data():
            return {"success": False, "error": "Invalid session or no data loaded"}

        df = session.data_session.df

        if column not in df.columns:
            return {"success": False, "error": f"Column '{column}' not found"}

        # Get value counts
        value_counts: pd.Series[int] | pd.Series[float]
        if normalize:
            value_counts = df[column].value_counts(
                normalize=True, sort=sort, ascending=ascending, dropna=False
            )
        else:
            value_counts = df[column].value_counts(
                normalize=False, sort=sort, ascending=ascending, dropna=False
            )

        # Apply top_n if specified
        if top_n:
            value_counts = value_counts.head(top_n)

        # Convert to dict
        counts_dict = {}
        for value, count in value_counts.items():
            if value is None or (isinstance(value, float) and pd.isna(value)):
                key = "NaN"
            else:
                key = str(value)
            counts_dict[key] = float(count) if normalize else int(count)

        # Calculate additional statistics
        unique_count = df[column].nunique(dropna=False)
        null_count = df[column].isna().sum()

        session.record_operation(
            OperationType.ANALYZE,
            {
                "type": "value_counts",
                "column": column,
                "normalize": normalize,
                "top_n": top_n,
            },
        )

        return {
            "success": True,
            "column": column,
            "value_counts": counts_dict,
            "unique_values": int(unique_count),
            "null_count": int(null_count),
            "total_count": len(df),
            "normalized": normalize,
        }

    except Exception as e:
        logger.error(f"Error getting value counts: {e!s}")
        return {"success": False, "error": str(e)}


async def detect_outliers(
    session_id: str,
    columns: list[str] | None = None,
    method: str = "iqr",
    threshold: float = 1.5,
    ctx: Context | None = None,  # noqa: ARG001
) -> dict[str, Any]:
    """Detect outliers in numeric columns.

    Args:
        session_id: Session identifier
        columns: Columns to check (None for all numeric)
        method: Detection method ('iqr', 'zscore', 'isolation_forest')
        threshold: Threshold for outlier detection (1.5 for IQR, 3 for z-score)
        ctx: FastMCP context

    Returns:
        Dict with outlier information
    """
    try:
        manager = get_session_manager()
        session = manager.get_session(session_id)

        if not session or not session.data_session.has_data():
            return {"success": False, "error": "Invalid session or no data loaded"}

        df = session.data_session.df

        # Select numeric columns
        if columns:
            missing_cols = [col for col in columns if col not in df.columns]
            if missing_cols:
                return {"success": False, "error": f"Columns not found: {missing_cols}"}
            numeric_df = df[columns].select_dtypes(include=[np.number])
        else:
            numeric_df = df.select_dtypes(include=[np.number])

        if numeric_df.empty:
            return {"success": False, "error": "No numeric columns found"}

        outliers = {}

        if method == "iqr":
            for col in numeric_df.columns:
                q1 = numeric_df[col].quantile(0.25)
                q3 = numeric_df[col].quantile(0.75)
                iqr = q3 - q1

                lower_bound = q1 - threshold * iqr
                upper_bound = q3 + threshold * iqr

                outlier_mask = (numeric_df[col] < lower_bound) | (numeric_df[col] > upper_bound)
                outlier_indices = df.index[outlier_mask].tolist()

                outliers[col] = {
                    "method": "IQR",
                    "lower_bound": float(lower_bound),
                    "upper_bound": float(upper_bound),
                    "outlier_count": len(outlier_indices),
                    "outlier_percentage": round(len(outlier_indices) / len(df) * 100, 2),
                    "outlier_indices": outlier_indices[:100],  # Limit to first 100
                    "q1": float(q1),
                    "q3": float(q3),
                    "iqr": float(iqr),
                }

        elif method == "zscore":
            for col in numeric_df.columns:
                z_scores = np.abs(
                    (numeric_df[col] - numeric_df[col].mean()) / numeric_df[col].std()
                )
                outlier_mask = z_scores > threshold
                outlier_indices = df.index[outlier_mask].tolist()

                outliers[col] = {
                    "method": "Z-Score",
                    "threshold": threshold,
                    "outlier_count": len(outlier_indices),
                    "outlier_percentage": round(len(outlier_indices) / len(df) * 100, 2),
                    "outlier_indices": outlier_indices[:100],  # Limit to first 100
                    "mean": float(numeric_df[col].mean()),
                    "std": float(numeric_df[col].std()),
                }

        else:
            return {"success": False, "error": f"Unknown method: {method}"}

        # Summary statistics
        total_outliers = sum(cast("int", info["outlier_count"]) for info in outliers.values())

        session.record_operation(
            OperationType.ANALYZE,
            {
                "type": "outlier_detection",
                "method": method,
                "threshold": threshold,
                "columns": list(outliers.keys()),
            },
        )

        return {
            "success": True,
            "method": method,
            "threshold": threshold,
            "outliers": outliers,
            "total_outliers": total_outliers,
            "columns_analyzed": list(outliers.keys()),
        }

    except Exception as e:
        logger.error(f"Error detecting outliers: {e!s}")
        return {"success": False, "error": str(e)}


async def profile_data(
    session_id: str,
    include_correlations: bool = True,
    include_outliers: bool = True,
    ctx: Context | None = None,
) -> dict[str, Any]:
    """Generate comprehensive data profile.

    Args:
        session_id: Session identifier
        include_correlations: Include correlation analysis
        include_outliers: Include outlier detection
        ctx: FastMCP context

    Returns:
        Dict with complete data profile
    """
    try:
        manager = get_session_manager()
        session = manager.get_session(session_id)

        if not session or not session.data_session.has_data():
            return {"success": False, "error": "Invalid session or no data loaded"}

        df = session.data_session.df

        profile = {
            "summary": {
                "row_count": len(df),
                "column_count": len(df.columns),
                "total_rows": len(df),
                "total_columns": len(df.columns),
                "memory_usage_mb": round(df.memory_usage(deep=True).sum() / (1024 * 1024), 2),
                "duplicate_rows": df.duplicated().sum(),
                "duplicate_percentage": round(df.duplicated().sum() / len(df) * 100, 2),
            },
            "columns": {},
        }

        # Analyze each column
        for col in df.columns:
            col_data = df[col]
            col_profile = {
                "dtype": str(col_data.dtype),
                "null_count": int(col_data.isna().sum()),
                "null_percentage": round(col_data.isna().sum() / len(df) * 100, 2),
                "unique_count": int(col_data.nunique()),
                "unique_percentage": round(col_data.nunique() / len(df) * 100, 2),
            }

            # Numeric column analysis
            if pd.api.types.is_numeric_dtype(col_data):
                col_profile["type"] = "numeric"
                col_profile["statistics"] = {
                    "mean": float(col_data.mean()),
                    "std": float(col_data.std()),
                    "min": float(col_data.min()),
                    "max": float(col_data.max()),
                    "25%": float(col_data.quantile(0.25)),
                    "50%": float(col_data.quantile(0.50)),
                    "75%": float(col_data.quantile(0.75)),
                    "skewness": float(cast("float", col_data.skew())),
                    "kurtosis": float(cast("float", col_data.kurtosis())),
                }
                col_profile["zeros"] = int((col_data == 0).sum())
                col_profile["negative_count"] = int((col_data < 0).sum())

            # Datetime column analysis
            elif pd.api.types.is_datetime64_any_dtype(col_data):
                col_profile["type"] = "datetime"
                non_null = col_data.dropna()
                if len(non_null) > 0:
                    col_profile["date_range"] = {
                        "min": str(non_null.min()),
                        "max": str(non_null.max()),
                        "range_days": (non_null.max() - non_null.min()).days,
                    }

            # Categorical/text column analysis
            else:
                col_profile["type"] = "categorical"
                value_counts = col_data.value_counts()
                col_profile["most_frequent"] = {
                    "value": (str(value_counts.index[0]) if len(value_counts) > 0 else None),
                    "count": int(value_counts.iloc[0]) if len(value_counts) > 0 else 0,
                }

                # String-specific analysis
                if col_data.dtype == "object":
                    str_lengths = col_data.dropna().astype(str).str.len()
                    if len(str_lengths) > 0:
                        col_profile["string_stats"] = {
                            "min_length": int(str_lengths.min()),
                            "max_length": int(str_lengths.max()),
                            "mean_length": round(str_lengths.mean(), 2),
                        }

            profile["columns"][col] = col_profile

        # Add correlations if requested
        if include_correlations:
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) >= 2:
                corr_result = await get_correlation_matrix(session_id, ctx=ctx)
                if corr_result["success"]:
                    profile["correlations"] = corr_result["significant_correlations"]

        # Add outlier detection if requested
        if include_outliers:
            outlier_result = await detect_outliers(session_id, ctx=ctx)
            if outlier_result["success"]:
                profile["outliers"] = {
                    col: {
                        "count": info["outlier_count"],
                        "percentage": info["outlier_percentage"],
                    }
                    for col, info in outlier_result["outliers"].items()
                }

        # Data quality score
        total_cells = len(df) * len(df.columns)
        missing_cells = df.isna().sum().sum()
        quality_score = round((1 - missing_cells / total_cells) * 100, 2)
        profile["data_quality_score"] = quality_score

        session.record_operation(
            OperationType.PROFILE,
            {
                "include_correlations": include_correlations,
                "include_outliers": include_outliers,
            },
        )

        return {"success": True, "profile": profile}

    except Exception as e:
        logger.error(f"Error profiling data: {e!s}")
        return {"success": False, "error": str(e)}
