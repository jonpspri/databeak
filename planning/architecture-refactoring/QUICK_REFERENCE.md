# Architecture Refactoring Quick Reference

## ğŸ“‹ Executive Summary

**Goal**: Transform DataBeak from CSV-focused tool to multi-format data platform
**Timeline**: 10-12 weeks in 3 phases
**Approach**: Incremental refactoring with mandatory regression testing

## ğŸ—ï¸ Current Architecture (Issues)

```
âŒ Current Problems:
â”œâ”€â”€ CSVSession (415 lines) - Too many responsibilities
â”œâ”€â”€ Tool Registration (~200 lines boilerplate) - Repetitive patterns
â”œâ”€â”€ Pandas Coupling - Hard dependencies throughout
â”œâ”€â”€ CSV-Only Input - Limited to CSV files for data loading
â””â”€â”€ Manual Configuration - Scattered settings management

âœ… Current Strengths:
â”œâ”€â”€ Session Management - Lifecycle, auto-save, history tracking
â”œâ”€â”€ Type Safety - Comprehensive Pydantic models
â”œâ”€â”€ MCP Integration - Clean separation of concerns
â”œâ”€â”€ Error Handling - Structured exception hierarchy
â””â”€â”€ Modular Tools - 40+ tools across 7 functional areas
```

## ğŸ¯ Target Architecture (Solutions)

```
âœ… Proposed Solutions:
â”œâ”€â”€ DataSource Abstraction - Protocol-based multi-format support
â”œâ”€â”€ Tool Decorator System - Auto-registration with @register_mcp_tool
â”œâ”€â”€ Session Hierarchy - Format-specific sessions with shared base
â”œâ”€â”€ Plugin Architecture - Extensible format support system
â””â”€â”€ Unified Configuration - pyproject.toml-based settings

ğŸš€ New Capabilities:
â”œâ”€â”€ Excel/XLSX - First-class multi-sheet workbook support
â”œâ”€â”€ Databases - SQLite, PostgreSQL, MySQL connectivity
â”œâ”€â”€ Streaming - Process 10GB+ files with <2GB memory
â”œâ”€â”€ Cross-Format - Transfer data between session types
â””â”€â”€ Auto-Discovery - Plugin system for third-party formats
```

## âš¡ Implementation Phases

### **Phase 1: Foundation (Weeks 1-3)**

```
Week 1: Data Source Abstraction (CRITICAL PATH)
â”œâ”€â”€ DataSource protocol design
â”œâ”€â”€ CSVDataSource refactoring
â”œâ”€â”€ Session management updates
â””â”€â”€ Plugin discovery mechanism

Week 2: Tool Registration Automation
â”œâ”€â”€ @register_mcp_tool decorator
â”œâ”€â”€ Auto-discovery system
â”œâ”€â”€ Boilerplate elimination
â””â”€â”€ Error handling standardization

Week 3: Testing & Validation
â”œâ”€â”€ Comprehensive regression tests
â”œâ”€â”€ Performance baseline validation
â”œâ”€â”€ Integration test updates
â””â”€â”€ Documentation refresh
```

### **Phase 2: Format Extensions (Weeks 4-7)**

```
Week 4-5: Excel Integration
â”œâ”€â”€ ExcelDataSource implementation
â”œâ”€â”€ Multi-sheet support (list, switch, merge)
â”œâ”€â”€ Excel-specific session type
â””â”€â”€ Workbook metadata extraction

Week 6-7: Database Connectivity
â”œâ”€â”€ SQLite foundation + PostgreSQL/MySQL
â”œâ”€â”€ Connection management with pooling
â”œâ”€â”€ SQL query execution with safety validation
â””â”€â”€ Schema introspection and table operations
```

### **Phase 3: Advanced Features (Weeks 8-10)**

```
Week 8: Streaming & Performance
â”œâ”€â”€ Chunked processing for large datasets
â”œâ”€â”€ Memory optimization improvements
â”œâ”€â”€ Progress tracking for long operations
â””â”€â”€ Async I/O enhancements

Week 9: Cross-Format Workflows
â”œâ”€â”€ Data transfer between session types
â”œâ”€â”€ Multi-source analysis pipelines
â”œâ”€â”€ Format conversion utilities
â””â”€â”€ Workflow orchestration tools

Week 10: Integration & Polish
â”œâ”€â”€ Configuration management enhancements
â”œâ”€â”€ Performance monitoring and diagnostics
â”œâ”€â”€ Error handling improvements
â””â”€â”€ Final documentation and examples
```

## ğŸ”’ Mandatory Testing Requirements

### **âš ï¸ BEFORE ANY REFACTORING**

```bash
# Required before starting each week:
uv run pytest --cov=src/databeak --cov-fail-under=90
uv run pytest tests/integration/ -v
uv run pytest tests/benchmarks/ --benchmark-only

# Create baseline measurements:
Performance baselines â†’ Store in baseline.json
Memory usage limits â†’ Document current usage patterns
Test coverage report â†’ Identify coverage gaps
```

### **Test Coverage Requirements**

- **>90% coverage** for areas being refactored
- **Integration tests** for end-to-end workflows
- **Performance baselines** with \<10% regression tolerance
- **Error condition tests** for all failure modes
- **Backward compatibility** validation for existing tools

## ğŸ›¡ï¸ Safety & Risk Mitigation

### **Rollback Strategy**

```
Safety Measures:
â”œâ”€â”€ Feature Flags - Enable/disable new formats individually
â”œâ”€â”€ Parallel Testing - Run old and new systems during transition
â”œâ”€â”€ Git History - All original code preserved in commits
â”œâ”€â”€ Performance Monitoring - Detect regressions immediately
â””â”€â”€ Gradual Rollout - Phased deployment with user feedback
```

### **High-Risk Areas**

- **Session Management**: Core to all operations (use adapter patterns)
- **Tool Interfaces**: Breaking changes affect all MCP tools
- **Database Security**: SQL injection, unauthorized access risks
- **Performance**: Abstraction overhead could slow operations

## ğŸ’¼ Business Value

### **Immediate Wins (Phase 1)**

- **30% code reduction** through boilerplate elimination
- **Simplified maintenance** with standardized patterns
- **Foundation for growth** enabling format extensions
- **Improved testability** with better abstractions

### **Format Expansion (Phase 2)**

- **Excel support** addresses major user requests
- **Database connectivity** enables enterprise adoption
- **Competitive advantage** over CSV-only tools
- **Expanded user base** through multi-format capabilities

### **Scalability (Phase 3)**

- **10GB+ file support** through streaming processing
- **Enterprise performance** with connection pooling
- **Advanced workflows** for power users
- **Production readiness** with monitoring and diagnostics

## ğŸ”§ Technical Implementation

### **Key Abstractions**

```python
# Core abstraction pattern:
class DataSource(Protocol):
    async def load(self, config: SourceConfig) -> DataFrame
    async def save(self, df: DataFrame, config: TargetConfig) -> SaveResult
    def get_capabilities(self) -> FormatCapabilities

# Format-specific implementations:
CSVDataSource    - Enhanced with streaming support
ExcelDataSource  - Multi-sheet workbook operations
DatabaseSource   - SQL execution with connection pooling

# Session hierarchy:
DataSession (base) â†’ CSVSession, ExcelSession, DatabaseSession
```

### **Tool Registration Automation**

```python
# Replace manual boilerplate:
@mcp.tool
async def filter_rows(ctx, session_id, conditions, logic) -> dict:
    # 20+ lines of boilerplate per tool

# With auto-registration:
@register_mcp_tool("data", "Filter rows based on conditions")
async def filter_rows(session_id: str, conditions: list[FilterCondition]) -> FilterResult:
    # Pure business logic only
```

## ğŸ“Š Success Metrics

### **Technical Metrics**

- [ ] **Multi-format support**: CSV, Excel, SQLite, PostgreSQL
- [ ] **Code reduction**: 30% less boilerplate code
- [ ] **Performance**: Maintain speed, add streaming for >1GB files
- [ ] **Memory efficiency**: Process 10x larger files with same memory
- [ ] **Test coverage**: >90% across all implementations

### **User Experience Metrics**

- [ ] **Zero breaking changes**: All existing workflows unchanged
- [ ] **Enhanced capabilities**: Excel and database tools available
- [ ] **Clear error messages**: Categorized, actionable feedback
- [ ] **Seamless adoption**: New formats work with existing tools

### **Development Metrics**

- [ ] **Reduced maintenance**: Fewer files to modify for changes
- [ ] **Faster development**: New tools easier to create
- [ ] **Better testing**: Isolated components with dependency injection
- [ ] **Enhanced modularity**: Lower coupling between components

## ğŸš€ Getting Started

### **Pre-Implementation Checklist**

1. **Get stakeholder approval** for refactoring approach
1. **Create feature branch** for Phase 1 work
1. **Set up regression test suite** (>90% coverage requirement)
1. **Establish performance baselines** for regression detection
1. **Configure feature flags** for gradual rollout

### **Week 1 Kickoff**

```bash
# Start with data source abstraction (critical path):
git checkout -b refactor/data-source-abstraction

# Establish test coverage:
uv run pytest --cov=src/databeak/models/ --cov-fail-under=90

# Begin implementation:
mkdir src/databeak/sources/
touch src/databeak/sources/{base.py,types.py,csv.py}
```

______________________________________________________________________

**This quick reference provides immediate access to key information from the
comprehensive planning documents, enabling developers to quickly understand
the refactoring strategy and implementation approach.**
